import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, errors
from datetime import datetime
import schedule
import time
import threading

MONGO_URI = "mongodb+srv://myuser:mypassword123!@cluster0.sqzxe33.mongodb.net/?appName=Cluster0"
client = MongoClient(MONGO_URI)
db = client["newsdb"]
collection = db["news"]

# link ì¤‘ë³µ ë°©ì§€ ì¸ë±ìŠ¤ ìƒì„±
collection.create_index("link", unique=True)


# ğŸ”¥ í•œ í˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜
def fetch_headlines(page):
    url = f"https://news.naver.com/section/105?page={page}"

    try:
        response = requests.get(
            url,
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=5
        )
        response.raise_for_status()
    except Exception as e:
        print("ìš”ì²­ ì˜¤ë¥˜:", e)
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    items = soup.select("li.sa_item")

    results = []

    for item in items:
        title_tag = item.select_one("a.sa_text_title")
        press_tag = item.select_one("div.sa_text_press")
        lede_tag = item.select_one("div.sa_text_lede")

        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)
        link = "https://news.naver.com" + title_tag.get("href")
        press = press_tag.get_text(strip=True) if press_tag else ""
        lede = lede_tag.get_text(strip=True) if lede_tag else ""

        results.append({
            "title": title,
            "link": link,
            "press": press,
            "lede": lede,
            "created_at": datetime.utcnow()
        })

    return results


# ğŸ”¥ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
def run_crawler():
    print("\n===== í¬ë¡¤ë§ ì‹œì‘ =====")
    try:
        for p in range(1, 6):
            print(f"í¬ë¡¤ë§ {p} í˜ì´ì§€")
            headlines = fetch_headlines(p)

            for item in headlines:
                try:
                    collection.insert_one(item)
                    print("ì €ì¥ë¨:", item["title"])
                except errors.DuplicateKeyError:
                    print("ì¤‘ë³µ ìŠ¤í‚µ:", item["title"])
    except Exception as e:
        print("í¬ë¡¤ëŸ¬ ì „ì²´ ì˜¤ë¥˜:", e)

    print("===== í¬ë¡¤ë§ ì¢…ë£Œ =====\n")


# ğŸ”¥ Render ë¬´ë£Œ Sleep ë°©ì§€ìš© ping
def keep_alive():
    try:
        requests.get("https://my-crawler-fv8n.onrender.com/", timeout=3)
        print("Keep-alive ìš”ì²­ ì „ì†¡")
    except Exception as e:
        print("Keep-alive ì‹¤íŒ¨:", e)


# ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹¤í–‰
def start_crawler_background():
    def job():
        print("ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘ë¨")

        # ìµœì´ˆ 1íšŒ ì‹¤í–‰
        run_crawler()

        # ë§¤ 1ë¶„ í¬ë¡¤ë§
        schedule.every(60).seconds.do(run_crawler)

        # Render Sleep ë°©ì§€
        schedule.every(10).minutes.do(keep_alive)

        while True:
            try:
                schedule.run_pending()
            except Exception as e:
                print("ìŠ¤ì¼€ì¤„ ì˜¤ë¥˜:", e)

            time.sleep(1)

    thread = threading.Thread(target=job)
    thread.daemon = True
    thread.start()
