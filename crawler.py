import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, errors
from datetime import datetime
import time

MONGO_URI = "mongodb+srv://myuser:mypassword123!@cluster0.sqzxe33.mongodb.net/?appName=Cluster0"

# -----------------------------
# ğŸš€ MongoDB ì—°ê²° (ì¬ì‹œë„ í¬í•¨)
# -----------------------------
def connect_mongo():
    while True:
        try:
            client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            client.server_info()  # ì—°ê²° í…ŒìŠ¤íŠ¸
            print("âœ… MongoDB ì—°ê²° ì„±ê³µ")
            return client
        except Exception as e:
            print("âŒ MongoDB ì—°ê²° ì‹¤íŒ¨. 5ì´ˆ í›„ ì¬ì‹œë„:", e)
            time.sleep(5)

client = connect_mongo()
db = client["newsdb"]
collection = db["news"]

collection.create_index("link", unique=True)


# -----------------------------
# ğŸš€ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
# -----------------------------
def fetch_headlines(page):
    url = f"https://news.naver.com/section/105?page={page}"

    try:
        response = requests.get(
            url,
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=8
        )
        response.raise_for_status()
    except Exception as e:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ (page {page}):", e)
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    items = soup.select("li.sa_item")

    results = []

    for item in items:
        title_tag = item.select_one("a.sa_text_title")
        press_tag = item.select_one("div.sa_text_press")
        lede_tag = item.select_one("div.sa_text_lede")

        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)
        link = "https://news.naver.com" + title_tag.get("href")
        press = press_tag.get_text(strip=True) if press_tag else ""
        lede = lede_tag.get_text(strip=True) if lede_tag else ""

        results.append({
            "title": title,
            "link": link,
            "press": press,
            "lede": lede,
            "created_at": datetime.utcnow()
        })

    return results


# -----------------------------
# ğŸš€ ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
# -----------------------------
def run_crawler():
    print("\n===== ğŸ”¥ í¬ë¡¤ë§ ì‹œì‘ =====\n")

    for p in range(1, 11):
        print(f"â¡ í˜ì´ì§€ {p} ìˆ˜ì§‘ ì¤‘...")
        headlines = fetch_headlines(p)

        for item in headlines:
            try:
                collection.insert_one(item)
                print("  âœ” ì €ì¥ë¨:", item["title"])
            except errors.DuplicateKeyError:
                print("  â†ª ì¤‘ë³µ ìŠ¤í‚µ:", item["title"])
            except Exception as e:
                print("  âŒ ì €ì¥ ì˜¤ë¥˜:", e)

    print("\n===== ğŸŸ¢ í¬ë¡¤ë§ ì™„ë£Œ =====\n")


# -----------------------------
# ğŸš€ Worker ë©”ì¸ ë£¨í”„ (1ë¶„ë§ˆë‹¤ ì‹¤í–‰)
# -----------------------------
if __name__ == "__main__":
    print("ğŸš€ Render Worker: ìë™ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘ (1ë¶„ë§ˆë‹¤ ë°˜ë³µ)")

    while True:
        try:
            run_crawler()
        except Exception as e:
            print("âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)

        print("â³ 60ì´ˆ ëŒ€ê¸°...\n")
        time.sleep(60)
