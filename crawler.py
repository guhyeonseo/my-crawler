import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, errors
from datetime import datetime
import schedule
import time
import threading

MONGO_URI = "mongodb+srv://myuser:mypassword123!@cluster0.sqzxe33.mongodb.net/?appName=Cluster0"
client = MongoClient(MONGO_URI)
db = client["newsdb"]
collection = db["news"]

collection.create_index("link", unique=True)


def fetch_headlines(page):
    url = f"https://news.naver.com/section/105?page={page}"
    response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    response.raise_for_status()

    soup = BeautifulSoup(response.text, "html.parser")
    items = soup.select("li.sa_item")

    results = []

    for item in items:
        title_tag = item.select_one("a.sa_text_title")
        press_tag = item.select_one("div.sa_text_press")
        lede_tag = item.select_one("div.sa_text_lede")

        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)
        link = "https://news.naver.com" + title_tag.get("href")
        press = press_tag.get_text(strip=True) if press_tag else ""
        lede = lede_tag.get_text(strip=True) if lede_tag else ""

        results.append({
            "title": title,
            "link": link,
            "press": press,
            "lede": lede,
            "created_at": datetime.utcnow()
        })

    return results


def run_crawler():
    print("\n===== í¬ë¡¤ë§ ì‹œì‘ =====")
    for p in range(1, 6):
        print(f"í¬ë¡¤ë§ {p} í˜ì´ì§€")
        headlines = fetch_headlines(p)

        for item in headlines:
            try:
                collection.insert_one(item)
                print("ì €ì¥ë¨:", item["title"])
            except errors.DuplicateKeyError:
                print("ì¤‘ë³µ ìŠ¤í‚µ:", item["title"])
    print("===== í¬ë¡¤ë§ ì¢…ë£Œ =====\n")


# ğŸ”¥ Render ë¬´ë£Œ í”Œëœ Sleep ë°©ì§€ìš© Ping í•¨ìˆ˜
def keep_alive():
    try:
        requests.get("https://my-crawler-fv8n.onrender.com/")
        print("Keep-alive ìš”ì²­ ì „ì†¡")
    except Exception as e:
        print("Keep-alive ì‹¤íŒ¨:", e)


def start_crawler_background():
    def job():
        # ìµœì´ˆ 1íšŒ í¬ë¡¤ë§ ì‹¤í–‰
        run_crawler()

        # 1ë¶„ë§ˆë‹¤ í¬ë¡¤ë§ ì‹¤í–‰
        schedule.every(60).seconds.do(run_crawler)

        # ğŸ”¥ 10ë¶„ë§ˆë‹¤ Keep-alive ì‹¤í–‰
        schedule.every(10).minutes.do(keep_alive)

        while True:
            schedule.run_pending()
            time.sleep(1)

    thread = threading.Thread(target=job)
    thread.daemon = True
    thread.start()
